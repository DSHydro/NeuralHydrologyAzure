name: NeuralHydrology - Hyperparameter search

permissions:
  id-token: write
  contents: read

on:
  workflow_dispatch:
    inputs:
      zipped_data_blob:
        description: 'blob name in Azure Storage'
        required: true
        type: string
        default: 'basin_timeseries_v1p2_metForcing_obsFlow.zip'

jobs:
  setup-computer-and-data:
    environment: AIForGood
    runs-on: T4GPU
    # Cancel if runs take longer than 6 hours
    timeout-minutes: 360

    steps:
      - name: AZ CLI login
        uses: azure/login@v2.3.0
        with:
          # NOTE: allow-no-subscripts needed if only giving scoped permissions to specific container
          allow-no-subscriptions: true
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}

      - uses: actions/checkout@v5

      - name: Report Runner Specs
        run: |
          lscpu
          free -h
          df -h

      - name: Report GPU specs
        run: |
          nvidia-smi

      - uses: prefix-dev/setup-pixi@v0.9.2
        with:
            manifest-path: fine-tuning/pyproject.toml
            cache: false #true
            # frozen: true  #do not want this in the first run

      # make sure netCDF4 is installed
      - name: Ensure netCDF4 is installed in GPU environment
        run: |
            pixi run -e gpu pip install netCDF4

      - name: Test GPU support
        run: |
            pixi run -e gpu python -c "import torch; print(torch.cuda.is_available())"

      # TODO: scope cache key with workflow name? otherwise can end up with incorrect data
      # - name: Cache Data
      #   uses: actions/cache@v4
      #   with:
      #       path: input-data.zip
      #       key: ${{ runner.os }}-input-data

      - name: Download Data from Azure Blob Storage
        run: |
            az storage blob download \
                --no-progress \
                --auth-mode login \
                --account-name dshydro\
                --container-name accelerator2025 \
                --name ${{ github.event.inputs.zipped_data_blob }} \
                --file input-data.zip > /dev/null

      - name: Setup Tutorial Data
        run: |
            unzip -q -n input-data.zip -d ./data/
            # mv ./data/basin_dataset_public_v1p2 ./data/CAMELS_US/

      # - name: Mount Azure File Share
      #   env:
      #     AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
      #   run: |
      #     mkdir /tmp/data
      #     sudo mount -t cifs //dshydro.file.core.windows.net/neurohydro /tmp/data -o vers=3.0,username=dshydro,password=$AZURE_STORAGE_ACCOUNT_KEY,dir_mode=0777,file_mode=0777,serverino,cache=strict,actimeo=60
      #     ls -l /tmp/data

      # - name: Setup Data Path
      #   run: |
      #     mkdir ./data
      #     ln -s /tmp/data/caravan ./data
    
      - name: Run Pre-training/Fine-tuning
        id: train
        run: |
            pixi run -e gpu python fine-tuning/finetuning.py #train 
            # OUTDIR=`ls runs/`
            OUTDIR=$(ls -1 runs/ | tr -d '\n')  # remove any newlines
            
            # echo "outdir<<EOF" >> $GITHUB_OUTPUT
            # echo "$OUTDIR" >> $GITHUB_OUTPUT
            # echo "EOF" >> $GITHUB_OUTPUT
            
            # echo "outdir=$OUTDIR" >> $GITHUB_OUTPUT

      # - name: Run Evaluation
      #   run: |
      #       pixi run eval --run-dir ./runs/${{ steps.train.outputs.outdir }}

      # - name: Visualize Results
      #   run: |
      #       pixi run visualize ./runs/${{ steps.train.outputs.outdir }}

      # note: names can't have slashes
      # - name: Upload Results
      #   uses: actions/upload-artifact@v5
      #   with:
      #       name: ${{ steps.train.outputs.outdir }}
      #       path: ./runs/${{ steps.train.outputs.outdir }}
      - name: Upload Results
        uses: actions/upload-artifact@v5
        with:
          name: results_${{ steps.train.outputs.outdir }}
          path: |
            ./runs/${{ steps.train.outputs.outdir }}
            ./evaluation_plots/${{ steps.train.outputs.outdir }}
