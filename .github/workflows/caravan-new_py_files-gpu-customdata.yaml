name: NeuralHydrology GPU Input Data

permissions:
  id-token: write
  contents: read

on:
  workflow_dispatch:
    inputs:
      zipped_data_blob:
        description: 'blob name in Azure Storage'
        required: true
        type: string
        default: 'basin_timeseries_v1p2_metForcing_obsFlow.zip'

jobs:
  train-on-gpu:
    environment: AIForGood
    runs-on: T4GPU
    # Cancel if runs take longer than 2 hours
    timeout-minutes: 120

    steps:
      - name: AZ CLI login
        uses: azure/login@v2.3.0
        with:
          # NOTE: allow-no-subscripts needed if only giving scoped permissions to specific container
          allow-no-subscriptions: true
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}

      - uses: actions/checkout@v5

      - name: Report Runner Specs
        run: |
          lscpu
          free -h
          df -h

      - name: Report GPU specs
        run: |
          nvidia-smi

      - uses: prefix-dev/setup-pixi@v0.9.2
        with:
            manifest-path: pyproject.toml
            cache: true
            frozen: true

      - name: Test GPU support
        run: |
            pixi run -e gpu python -c "import torch; print(torch.cuda.is_available())"

      # TODO: scope cache key with workflow name? otherwise can end up with incorrect data
      # - name: Cache Data
      #   uses: actions/cache@v4
      #   with:
      #       path: input-data.zip
      #       key: ${{ runner.os }}-input-data

      - name: Download Data from Azure Blob Storage
        run: |
            az storage blob download \
                --no-progress \
                --auth-mode login \
                --account-name dshydro\
                --container-name accelerator2025 \
                --name ${{ github.event.inputs.zipped_data_blob }} \
                --file input-data.zip > /dev/null

      - name: Setup Tutorial Data
        run: |
            unzip -q -n input-data.zip -d ./data/
            mv ./data/basin_dataset_public_v1p2 ./data/CAMELS_US/

      - name: Run Training/Evaluation/Visualize Results
        id: train
        run: |
            pixi run train-gpu
            OUTDIR=`ls runs/`
            echo "outdir=$OUTDIR" >> $GITHUB_OUTPUT

      # - name: Run Evaluation
      #   run: |
      #       pixi run eval --run-dir ./runs/${{ steps.train.outputs.outdir }}

      # - name: Visualize Results
      #   run: |
      #       pixi run visualize ./runs/${{ steps.train.outputs.outdir }}

      # note: names can't have slashes
      - name: Upload Results
        uses: actions/upload-artifact@v5
        with:
            name: ${{ steps.train.outputs.outdir }}
            path: ./runs/${{ steps.train.outputs.outdir }}
