{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4524618",
   "metadata": {},
   "source": [
    "The idea with this nootebooks is to try a LOCI alike algorithm to fix caravan precipitation. The correcion would take the next steps:\n",
    "* Select the days when it actually rains on CAMELS\n",
    "* Continue with the rest of the correction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cfddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febfcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Both datasets range froom 1989-01-01 to 2019-12-31 so no need to slice by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da6a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 11 basins\n",
      "\n",
      "▶ Processing basin CAMELS_UY_2\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_2.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_3\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_3.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_5\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_5.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_6\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_6.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_7\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_7.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_8\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_8.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_9\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_9.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_10\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_10.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_11\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_11.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_15\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_15.nc\n",
      "\n",
      "▶ Processing basin CAMELS_UY_16\n",
      "  ✅ Saved to new_correction_approach/time_series/CAMELS_UY_16.nc\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Paths ----------------\n",
    "data_caravan = Path(\"../preparing_data/filtered_data/time_series\")\n",
    "data_gauge  = Path(\"../preparing_data/cleaned_filtered_data_gauge_precip/time_series\")\n",
    "output_dir   = Path(\"./new_correction_approach/time_series\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # ---------------- Dates ----------------\n",
    "# start_date = pd.Timestamp(\"1989-10-01\")\n",
    "# end_date   = pd.Timestamp(\"2008-09-30\")\n",
    "\n",
    "eps = 1e-6\n",
    "time_dim = \"date\"\n",
    "\n",
    "# ---------------- Load basin list ----------------\n",
    "with open(\"filtered_uy_basins.txt\") as f:\n",
    "    basins = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Processing {len(basins)} basins\")\n",
    "\n",
    "# ---------------- Loop over basins ----------------\n",
    "for basin in basins:\n",
    "    print(f\"\\n▶ Processing basin {basin}\")\n",
    "\n",
    "    # -------- Caravan file --------\n",
    "    caravan_file = data_caravan / f\"{basin}.nc\"\n",
    "    if not caravan_file.exists():\n",
    "        print(f\"  ⚠️ Caravan file not found, skipping\")\n",
    "        continue\n",
    "\n",
    "    caravan = xr.open_dataset(caravan_file)\n",
    "\n",
    "    # -------- Caravan file --------\n",
    "    gauge_file = data_gauge / f\"{basin}.nc\"\n",
    "    if not gauge_file.exists():\n",
    "        print(f\"  ⚠️ Gauge file not found, skipping\")\n",
    "        continue\n",
    "\n",
    "    gauge = xr.open_dataset(gauge_file)\n",
    "\n",
    "    # # -------- Time slice --------\n",
    "    # caravan = caravan.sel(date=slice(start_date, end_date))\n",
    "    # gauge = gauge.sel(date=slice(start_date, end_date))\n",
    "\n",
    "    # -------- Select variables --------\n",
    "    vars_keep = [\n",
    "        \"prcp_mm_day\",\n",
    "        \"tmax_C\",\n",
    "        \"tmin_C\",\n",
    "        \"srad_W_m2\",\n",
    "        \"QObs_mm_d\",\n",
    "    ]\n",
    "\n",
    "    # -------- Wet-day mask from observations --------\n",
    "    mask = gauge[\"prcp_mm_day\"] > 0\n",
    "\n",
    "    caravan_masked = caravan.copy()\n",
    "\n",
    "    caravan_masked[\"prcp_mm_day\"] = (\n",
    "        caravan_masked[\"prcp_mm_day\"].where(mask, 0)\n",
    "    )\n",
    "\n",
    "    # -------- Define obs / era precipitation --------\n",
    "    pr_era = caravan_masked[\"prcp_mm_day\"]\n",
    "\n",
    "    pr_obs = gauge[\"prcp_mm_day\"]\n",
    "\n",
    "    # -------- Add month coordinate --------\n",
    "    pr_obs = pr_obs.assign_coords(month=pr_obs[time_dim].dt.month)\n",
    "    pr_era = pr_era.assign_coords(month=pr_era[time_dim].dt.month)\n",
    "\n",
    "    # -------- Monthly means --------\n",
    "    obs_monthly_mean = pr_obs.groupby(\"month\").mean(time_dim)\n",
    "    era_monthly_mean = pr_era.groupby(\"month\").mean(time_dim)\n",
    "\n",
    "    # -------- Scaling factor --------\n",
    "    scaling_factor = obs_monthly_mean / (era_monthly_mean + eps)\n",
    "\n",
    "    # -------- Apply LS correction --------\n",
    "    pr_era_ls = pr_era.groupby(f\"{time_dim}.month\") * scaling_factor\n",
    "\n",
    "    # -------- Store back into dataset --------\n",
    "    caravan[\"prcp_mm_day\"] = pr_era_ls.drop_vars(\n",
    "        \"month\", errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # -------- Save --------\n",
    "    output_path = output_dir / f\"{basin}.nc\"\n",
    "    caravan.to_netcdf(output_path)\n",
    "\n",
    "    caravan.close()\n",
    "    gauge.close()\n",
    "\n",
    "    print(f\"  ✅ Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facc7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3932\n",
      "Total acumulated precip: 45184.78\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_dataset(data_caravan / \"CAMELS_UY_2.nc\")\n",
    "# ds = ds.sel(date=slice(start_date, end_date))\n",
    "\n",
    "n_zeros = (ds[\"prcp_mm_day\"] == 0).sum().item()\n",
    "total_precip = ds[\"prcp_mm_day\"].sum(dim=\"date\").values\n",
    "\n",
    "print(n_zeros)\n",
    "print('Total acumulated precip:',total_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b21382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7832\n",
      "Total acumulated precip: 49271.7\n"
     ]
    }
   ],
   "source": [
    "ds_gauge = xr.open_dataset(data_gauge / \"CAMELS_UY_2.nc\")\n",
    "# ds_gauge = ds_gauge.sel(date=slice(start_date, end_date))\n",
    "\n",
    "n_zeros_gauge = (ds_gauge[\"prcp_mm_day\"] == 0).sum().item()\n",
    "total_precip_gauge = ds_gauge[\"prcp_mm_day\"].sum(dim=\"date\").values\n",
    "\n",
    "print(n_zeros_gauge)\n",
    "print('Total acumulated precip:',total_precip_gauge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401d230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7948\n",
      "Total acumulated precip: 49271.688\n"
     ]
    }
   ],
   "source": [
    "ds_corrected = xr.open_dataset(output_dir / \"CAMELS_UY_2.nc\")\n",
    "# ds_corrected = ds_corrected.sel(date=slice(start_date, end_date))\n",
    "\n",
    "n_zeros_corrected = (ds_corrected[\"prcp_mm_day\"] == 0).sum().item()\n",
    "total_precip_corrected = ds_corrected[\"prcp_mm_day\"].sum(dim=\"date\").values\n",
    "\n",
    "print(n_zeros_corrected)\n",
    "print('Total acumulated precip:',total_precip_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
